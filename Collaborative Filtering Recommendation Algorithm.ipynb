{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Recommendation Algorithm\n",
    "## Div Dasani\n",
    "\n",
    "This algorithm employs the ALS implicit feedback approach to collaborative filtering descibed by [Hu et al.](http://yifanhu.net/PUB/cf.pdf) on Instacart order data to recommend products to users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Setup\n",
    "Instructions: In order to run this notebook, make sure you have all of the below packages installed. In particular, *implicit* can be installed via the command:\n",
    "```bash\n",
    "conda install -c conda-forge implicit\n",
    "```\n",
    "Additionally, please make sure [this](https://www.instacart.com/datasets/grocery-shopping-2017) Instacart data is downloaded and placed in the same directory as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "import random\n",
    "import implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Hyperparameters\n",
    "Below are the hyperparameters that govern the algorithm. They will be referenced throughout the notebook as necessary.\n",
    "<br>\n",
    "*train_ratio*- denotes the percentage of data that will be utilized for training\n",
    "<br>\n",
    "*factors*- denotes the number of latent factors assumed by the algorithm\n",
    "<br>\n",
    "*regularization*- denotes the magnitude of the regularization parameter, $\\lambda$\n",
    "<br>\n",
    "*iterations*- denotes the number of iterations run by the algorithm\n",
    "<br>\n",
    "*alpha*- denotes the learning rate of confidence of the algorithm\n",
    "<br>\n",
    "*N*- denotes the recommendation list size for calculating the *Mean Percentile Ranking (MPR)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "factors = 20\n",
    "regularization = 0.1\n",
    "iterations = 20\n",
    "alpha = 15\n",
    "N = 10**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Data Cleansing\n",
    "First, the data is loaded into the notebook as Pandas DataFrames. Then, the *user_id* values are joined with the *product_id* values into one DataFrame through the *order_id* key. After this, a *count* column is added, and identical instances of <*user_id*, *product_id*> are grouped together by incrementing the count of the row. Thus, all <*user_id*, *product_id*> tuples in the DataFrame are unique. Finally, the data is split into its training and testing components. A product dictionary DataFrame mapping product ids to product descriptions is also created for returning the names of recommended products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product_dic = pd.read_csv('products.csv')\n",
    "product_dic = product_dic[['product_id','product_name']]\n",
    "\n",
    "df_users = pd.read_csv('orders.csv')\n",
    "df_users = df_users[df_users['eval_set'] == 'prior']\n",
    "df_users = df_users[['order_id','user_id']]\n",
    "\n",
    "df_products = pd.read_csv('order_products__prior.csv')\n",
    "df_products = df_products[['order_id','product_id']]\n",
    "\n",
    "df_merged = pd.merge(df_users, df_products, on='order_id', how='left')\n",
    "df_merged = df_merged[['user_id','product_id']]\n",
    "df_merged['count'] = 1\n",
    "df_merged = df_merged.groupby(['user_id','product_id'], as_index=False)['count'].sum()\n",
    "\n",
    "df_train = df_merged.sample(frac=train_ratio)\n",
    "df_test = (df_merged.merge(df_train, on=['user_id','product_id'], how='left', indicator=True)\n",
    "           .query('_merge == \"left_only\"').drop('_merge', 1))\n",
    "df_test = df_test.drop(['count_y'],axis=1)\n",
    "df_test.columns = ['user_id', 'product_id', 'count_purchase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. The Model\n",
    "There are two types of data one can employ when making recommendations: explicit and implicit data. The former is feedback provided directly by the user that informs us of her opinion of the product (i.e. ratings). The latter is information that is indirectly generated by the user and may possess some correlation to the user's opinion of a product. This dataset is implicit, as we are using the number of times a user purchases a product to inform us of her preferences.\n",
    "<br>\n",
    "<br>\n",
    "Thus, our data can be represented as a matrix $R\\inℝ^{u*i}$ whose rows denote users ($u$) and whose columns denote products ($i$). The values of each cell in the matrix represent the number of times a particular product is purchased by a particular user, $r_{u,i}$. Because there are thousands of products and each user only purchases a fraction of these, $R$ is highly sparse. The way the algorithm learns from this missing data is through factorizing this data into two matrices $X\\inℝ^{u*f}$ and $Y\\inℝ^{f*i}$ where $f$ represents *factors*, the number of latent factors. Thus, each value in $X$ is a weight for a feature for each user and each value in $Y$ is a weight for a feature for each product. The goal is to tune the weights in each of these matrices such that $X^TY\\approx R$.\n",
    "<br>\n",
    "<br>\n",
    "The algorithm that tunes these weights is the *Alternating Least Squares (ALS)* algorithm. This algorithm works by alternatively tuning each matrix $X$ and $Y$ while holding the other constant. The algorithm is defined as,\n",
    "<center>$\\min\\limits_{X*,Y*}\\sum_{u,i} [c_{u,i}(p_{u,i} - X_{u}^TY_{i})^2] + \\lambda(\\sum_{u}||X_{u}||^2 + \\sum_{i}||Y_{i}||^2)$</center>\n",
    "where $p$ is the preference function given by,\n",
    "<center>$$ p_{u,i}(r_{u,i})=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & r_{u,i}>0 \\\\\n",
    "      0 & r_{u,i}=0 \\\\\n",
    "\\end{array} \n",
    "\\right.  $$</center>\n",
    "$c$ is the confidence function with learning rate *alpha* given by,\n",
    "<center>$c_{u,i} = 1 + \\alpha*r_{u,i}$</center>\n",
    "and $\\lambda$ represents the regularization parameter *regularization*, used to prevent overfitting by penalizing heavy weights.\n",
    "\n",
    "Below the model is implemented as described above. First, two sparse matrices are created: the item-user matrix is used to train the algorithm, while the user-item matrix will be used to extract recommendations for a given user from the trained model.The former matrix is multiplied by the confidence learning rate *alpha* before being fed to the ALS algorithm with prespecified parameters *factors* and *regularization* as described above, and minimizes the loss function over a prespecified number of iterations *iterations*. This notebook employs the implicit package to implement the ALS algorithm due to its ability to run calculations in C++, vastly increasing computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(factors,regularization,iterations,alpha):\n",
    "    sparse_item_user = sparse.csr_matrix((df_train['count'].astype(float), (df_train['product_id'], df_train['user_id'])))\n",
    "    sparse_user_item = sparse.csr_matrix((df_train['count'].astype(float), (df_train['user_id'], df_train['product_id'])))\n",
    "    data_conf = (sparse_item_user * alpha).astype('double')\n",
    "    \n",
    "    model = implicit.als.AlternatingLeastSquares(factors=factors, regularization=regularization, iterations=iterations)\n",
    "    model.fit(data_conf)\n",
    "    \n",
    "    return model, sparse_user_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intel MKL BLAS detected. Its highly recommend to set the environment variable 'export MKL_NUM_THREADS=1' to disable its internal multithreading\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 20.0/20 [01:35<00:00,  4.70s/it]\n"
     ]
    }
   ],
   "source": [
    "model, sparse_user_item = build_model(factors,regularization,iterations,alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Output Functions\n",
    "The *recommendations* method employs the user-item matrix along with a user id to recommend N products that the trained model believes the user has the highest chance of purchasing.\n",
    "<br>\n",
    "The *similar_products* method takes in a product id and returns N products that are the most similar to the product based on the trained model.\n",
    "<br>\n",
    "The *name* method takes in a list of product ids and returns the names of the products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name(product_ids):\n",
    "    product_names = []\n",
    "    for product_id in product_ids:\n",
    "        product_names.append(product_dic.product_name.loc[product_dic.product_id == product_id].iloc[0])\n",
    "    return pd.DataFrame({'product': product_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similar_products(product_id, N, named=False):\n",
    "    similar = model.similar_items(product_id, N+1)\n",
    "    \n",
    "    recs = []\n",
    "    for item in similar[1:]:\n",
    "        recs.append(item[0])\n",
    "    if named:\n",
    "        return name(recs)\n",
    "    else:\n",
    "        return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommendations(user_id, N, named=False):\n",
    "    recommended = model.recommend(user_id, sparse_user_item, N=N)\n",
    "    \n",
    "    recs = []\n",
    "    for item in recommended:\n",
    "        recs.append(item[0])\n",
    "    if named:\n",
    "        return name(recs)\n",
    "    else:\n",
    "        return recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Predictive Ability\n",
    "Now that the model has been trained, we will analyze its predictive ability. Below is an instance of a single user, whose purchase history is illustrated. This user tends to purchase health-oriented items, such as \"Organic Quick Oats\" and \"Michigan Organic Kale\". As a result, the algorithm recommends items in this category, such as \"Organic Riced Cauliflower\" and \"Organic Spaghetti Squash\". The recommendation system also nudges the user to products that are incredibly similar to previously purchased products; for example, the user purchased \"Dairy Free Unsweetened Vanilla Coconut Milk\", so the algorithm recommended \"Organic Unsweetened Vanilla Almond Milk\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Purchase History\n",
      "                                            product\n",
      "0                 Vanilla Almond Breeze Almond Milk\n",
      "1                         Organic Turkey Bone Broth\n",
      "2          All Natural No Stir Creamy Almond Butter\n",
      "3                      Sustainably Soft Bath Tissue\n",
      "4               Homestyle Savory Chicken Bone Broth\n",
      "5                   100% Organic Raw Coconut Butter\n",
      "6        Organic Raw Unfiltered Apple Cider Vinegar\n",
      "7         Renew Life Total Body 7-Day Rapid Cleanse\n",
      "8                 Natural Calm Magnesium Supplement\n",
      "9              All Natural Cocoa Powder Unsweetened\n",
      "10                 Cilantro Avocado Yogurt Dressing\n",
      "11   Dressing & Sandwich Spread Made with Olive Oil\n",
      "12                                      Dry Shampoo\n",
      "13                                    Garlic Powder\n",
      "14                   Italian Extra Virgin Olive Oil\n",
      "15                      Nutritional Yeast Seasoning\n",
      "16              French Vanilla Coconut Milk Creamer\n",
      "17             Grade A Large Eggs Cage Free Omega 3\n",
      "18                           Organic Chopped Garlic\n",
      "19                                          Carrots\n",
      "20                         Organic Broccoli Florets\n",
      "21                               Organic Quick Oats\n",
      "22   My Community Immune Support Dietary Supplement\n",
      "23          Organic Lightly Salted Brown Rice Cakes\n",
      "24                                    Turmeric Root\n",
      "25                       Creamy & Raw Almond Butter\n",
      "26      Dairy Free Unsweetened Vanilla Coconut Milk\n",
      "27                            Michigan Organic Kale\n",
      "28               Raw Manuka Honey Hi Active 15 Plus\n",
      "29                                Natural Sweetener\n",
      "30                           Organic Chopped Ginger\n",
      "31                 Organic High Fiber Coconut Flour\n",
      "32                               Organic Egg Whites\n",
      "33     Organic Gut Shot Ginger Beet Probiotic Drink\n",
      "34                        Broccoli Slaw Traditional\n",
      "35                             Dark Chocolate Chips\n",
      "36                 Original Unflavored Gelatine Mix\n",
      "37  Organic Lightly Sweetened Sunflower Seed Butter\n",
      "38                                  Black Eyed Peas\n",
      "39                          Classic Blend Cole Slaw\n",
      "40                   Naturals Lite Goddess Dressing\n",
      "41                         Organic Spring Mix Salad\n",
      "42               Organic Balsamic Vinegar Of Modena\n",
      "43                                   Coconut Butter\n",
      "44                            Squeezed Orange Juice\n",
      "45         Argan Oil Cleansing Towelettes Unscented\n",
      "46                         Wild Alaskan Pink Salmon\n",
      "\n",
      "Recommended Products\n",
      "                                   product\n",
      "0                Organic Riced Cauliflower\n",
      "1  Organic Unsweetened Vanilla Almond Milk\n",
      "2                      Apple Cider Vinegar\n",
      "3                 Organic Spaghetti Squash\n",
      "4           Organic Raw Kombucha Gingerade\n"
     ]
    }
   ],
   "source": [
    "user_id = 202279\n",
    "\n",
    "print('User Purchase History')\n",
    "print(name(df_merged[df_merged['user_id'] == user_id].product_id.tolist()))\n",
    "print('\\nRecommended Products')\n",
    "print(recommendations(user_id,5,named=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is also able to understand what products are similar to one another. In the example below, the algorithm is provided with \"Chocolate Sandwich Cookies\" and returns several similar snacks, such as \"Bite Size Candies, Original\" and \"Cheez-It Cheddar Cracker\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given Product: Chocolate Sandwich Cookies\n",
      "\n",
      "Similar Products:\n",
      "                       product\n",
      "0   Cup Noodles Chicken Flavor\n",
      "1  Bite Size Candies, Original\n",
      "2                Juice Squeeze\n",
      "3  Original Snack Mix 40 Ounce\n",
      "4     Cheez-It Cheddar Cracker\n"
     ]
    }
   ],
   "source": [
    "target = \"Chocolate Sandwich Cookies\"\n",
    "\n",
    "target_id = product_dic.product_id.loc[product_dic.product_name == target].iloc[0]\n",
    "print('Given Product: {}'.format(target))\n",
    "print('\\nSimilar Products:')\n",
    "print(similar_products(target_id,5,named=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall accuracy of the recommendation algorithm on the test data is judged by the Mean Percentile Ranking (MPR) metric. This value is given by,\n",
    "<center> $\\bar{rank} = \\frac{\\sum_{u,i}r^t_{u,i}rank_{u,i}}{\\sum_{u,i}r^t_{u,i}}$ </center>\n",
    "where $r^t_{u,i}$ represents the number of times a product $i$ is purchased by a user $u$ in the test set, and $rank_{u,i}$ is a percentage denoting how highly a product $i$ is ranked for user $u$ in the recommendation list, where $0%$ denotes a product being recommended at the top of the list. The MPR is bounded by $1$, and the expected MPR of a system that recommends products at random is $0.5$, meaning that a system with a higher value than this is essentially no better than random. This recommendation system received an MPR score of $0.2376$.\n",
    "<br>\n",
    "<br>\n",
    "The implication of MPR is that the recommendation system provide a ranking for every product for each user. However, this process is immensely expensive, so I have modified the method in this notebook to create a list of only the *N* most likely recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_mpr(N):\n",
    "    mpr = []\n",
    "    test_users = set(df_test.user_id.tolist())\n",
    "    for user_id in test_users:\n",
    "        mpr_user = 0\n",
    "\n",
    "        temp_df = df_test.loc[df_test['user_id'] == user_id]\n",
    "        products = temp_df.product_id.tolist()\n",
    "        count = temp_df.count_purchase.tolist()\n",
    "\n",
    "        recs = recommendations(user_id, N)\n",
    "        for i in range(len(products)):\n",
    "            if products[i] in recs:\n",
    "                rank = recs.index(products[i])\n",
    "                mpr_user+=(rank/N)*count[i]\n",
    "        mpr.append(mpr_user/sum(count))\n",
    "    return mpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MPR: 0.2376\n"
     ]
    }
   ],
   "source": [
    "mpr = calculate_mpr(N)\n",
    "print('Test MPR: {:.4f}'.format(np.mean(mpr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G. Sources\n",
    "1. [Hu et al., Collaborative Filtering for Implicit Feedback Datasets](http://yifanhu.net/PUB/cf.pdf)\n",
    "2. [Building a Recommendation System in TensorFlow: Overview](https://cloud.google.com/solutions/machine-learning/recommendation-system-tensorflow-overview)\n",
    "3. [implicit](https://github.com/benfred/implicit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
